import { PromptTemplate } from "@langchain/core/prompts"
import type { NextApiRequest, NextApiResponse } from 'next'

import { answerTemplate, standaloneQuestionTemplate } from "@/templates"
import { combineDocuments } from "@/util/combineDocument"
import { getRemoteRetriver } from "@/util/remoteRetriever"
import { StringOutputParser } from '@langchain/core/output_parsers'
import { RunnablePassthrough, RunnableSequence } from "@langchain/core/runnables"
import { ChatOpenAI } from "@langchain/openai"

import { getLocalRetriever } from "@/util/localRetriever"
import fs from "fs"
import { VectorStoreRetriever } from "langchain/vectorstores/base"

 
export default async  function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {

  const {conversation_history, question} = req.body
  const openAIApiKey = process.env.OPENAI_API_KEY
  const llm = new ChatOpenAI({ openAIApiKey })
  const VECTOR_STORE_PATH = "store";
  let retriever: VectorStoreRetriever;
  
  console.log("ðŸ” Checking for existing vector store...");
  
   // ë²¡í„° ìŠ¤í† ì–´ê°€ ë¡œì»¬ì— ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.  
  if (fs.existsSync(VECTOR_STORE_PATH)) {
    console.log("ðŸ“š Vector store found, using local retriever...");
    retriever = await getLocalRetriever()
  } else {
    console.log("ðŸŒ Vector store not found, using remote retriever...");
    retriever = await getRemoteRetriver()
  }
  
  // ë…ë¦½ì  ì§ˆë¬¸ ìƒì„± 
  const standaloneQuestionChain = createStandaloneQuestionChain(llm);

  // ë¦¬íŠ¸ë¦¬ë²„ ì²´ì¸ ìƒì„±
  const retrieverChain = createRetrieverChain(retriever);

  // ë‹µë³€ ì²´ì¸ ìƒì„±
  const answerChain = createAnswerChain(llm);
      
  console.log("ðŸ”— Building the runnable sequence chain...");

  // ì²´ì´ë‹ ìƒì„±
  const chain = RunnableSequence.from([
  {
      standalone_question: standaloneQuestionChain,
      original_input: new RunnablePassthrough()
  },
  {
      context: retrieverChain,
      question: ({original_input}) => original_input.question,
      conversation_history: ({original_input}) => original_input.conversation_history
  },
  answerChain
  ])

  console.log("ðŸš€ Invoking the runnable sequence chain...");

  // ì²´ì´ë‹ ì‹¤í–‰
  const response = await chain.invoke({
    question,
    conversation_history
  })
  console.log("âœ… Runnable sequence chain invoked successfully!");

  res.status(200).json({ response })
    
}

function createStandaloneQuestionChain(llm: ChatOpenAI) {
  const standaloneQuestionPrompt = PromptTemplate.fromTemplate(standaloneQuestionTemplate);
  return standaloneQuestionPrompt
    .pipe(llm)
    .pipe(new StringOutputParser());
}

function createRetrieverChain(retriever: VectorStoreRetriever) {
  return RunnableSequence.from([
    prevResult => prevResult.standalone_question,
    retriever,
    combineDocuments
  ]);
}

function createAnswerChain(llm: ChatOpenAI) {
  const answerPrompt = PromptTemplate.fromTemplate(answerTemplate);
  return answerPrompt
    .pipe(llm)
    .pipe(new StringOutputParser());
}
